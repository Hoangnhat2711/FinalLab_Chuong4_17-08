{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Colab ********************************\n",
    "#Code này nếu chạy trên Google Colab sẽ xuất luôn file vào folder nhưng không tải về được nên sẽ add vào hết 1 text cách nhau bởi \"$$$\" sau đó vào add file text đó vào VS Code để tách ra và lưu vào thư mục\n",
    "text=''\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from underthesea import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from autocorrect import spell\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from nltk.probability import FreqDist\n",
    "file_list = os.listdir(\"Data2\")\n",
    "# Duyệt qua từng file txt\n",
    "for file_name in file_list:\n",
    "    # Mở file txt\n",
    "    with open(os.path.join(\"Data2\", file_name), \"r\") as f:\n",
    "      text = f.read()\n",
    "    def converting_emojis(text):\n",
    "        for x in EMOTICONS_EMO:\n",
    "            text = text.replace(x, \"_\".join(EMOTICONS_EMO[x].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        for x in UNICODE_EMOJI:\n",
    "            text = text.replace(x, \"_\".join(UNICODE_EMOJI[x].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        return text\n",
    "    all_words= []\n",
    "    text = text.lower() # Convert text to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove html\n",
    "    text = re.sub(r'[^\\w\\s]','',text) # Remove punctuation\n",
    "    text = re.sub(\"\\d+\", \" \", text) # Remove number\n",
    "    text = converting_emojis(text) #Remove emoji\n",
    "    text = re.sub(r\"[!@#$[]()]'\", \"\", text) # Remove character: !@#$[]()\n",
    "    with open(\"vietnamese-stopwords.txt\", \"r\") as f:\n",
    "    #Get Stop words Dictionaries\n",
    "      List_StopWords=f.read().split(\"\\n\")\n",
    "    #remove stop words\n",
    "    text_pre=\" \".join(text for text in text.split() if text not in List_StopWords) #Remove stopwords\n",
    "    # text=spell(text)\n",
    "    sentences = nltk.sent_tokenize(text) # Sentence Tokenizing\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence) # Word Tokenizing\n",
    "        all_words.extend(words)\n",
    "\n",
    "    folder_path = '/content/Data_Pre_Processing'\n",
    "\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for word in all_words:\n",
    "            output_file.write(word + '\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
